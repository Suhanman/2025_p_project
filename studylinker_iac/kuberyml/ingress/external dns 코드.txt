

31.재해복구에 대해서 설명드리겠습니다.
저희는 Route 53 헬스체크를 통한 트래픽 전환과 kubernetes Cronjob을 통해서 DB의 주기적 SQL 덤프를 생성하고 이를 gsuitl을 통해서 
gcp의 GCS로 전송함으로써 장애 발생시 생기는 데이터 손실과 웹 페이지 접속 장애를 대비하고자 하였습니다.
우선 트래픽 기반 장애 조치(Failover) 도메인 라우팅 구성에 대해 설명드리도록 하겠습니다.

32. Route53의 상태검사(헬스체크)와 Failover 라우팅 정책을 활용하였습니다.
우선 상태검사를 생성하여 해당 상태검사를 각각 기존 레코드와 보조 레코드에 상태확인 id로 등록합니다.
이때 기존 레코드는 aws 의 ingress dns이고 보조 레코드는 구글의 스태틱 ip 입니다.
이렇게 라우팅 정책을 구성하면 생성한 상태검사의 헬스체크가 unhealthy 가 뜰 시 
자동으로 lovebridge.click 은 aws가 아닌 구글의 스태틱 아이피로 호스팅 하고 이를 확인할 수 있었습니다.

33~35. Cronjob을 이용한 DB 백업 구조에 대해 설명드리겠습니다.

Dockerfile을 기반으로 한 백업 시스템인데 aws의 rds의 사용되는 db에서 .sql 파일로 디비의 덤프를 추출하는 도커파일을 생성하여 
이를 실행하는 sh 파일과 함께 도커 이미지화 하여 ECR에 올립니다. 
해당 이미지를 kubernetes의 Cronjob 에서 주기적으로 실행하여 gcp 스토리지로 전송하고 전과 유서하게 해당 sql 파일을 gcp의 CloudSql 로 
다운 받는 도커이미지를 생성해 역시 Cronjob에서 주기적으로 실행합니다.


37~38. 모니터링에 대해 설명드리겠습니다.

모니터링의 경우 프로메테우스 + 그라파나를 사용하여서 시각화까지 구현해보았습니다.
aws 내부 지표를 모니터링하기 위해 cloudwatch-exporter와
kubernetes 를 모니터링을 위한 kube-state-metrics, gcp 모니터링을 위한 gcp-exporter 등을 활용하여 다양한 매트릭을 수집하였고
이를 grafana와 연동하였습니다. 또한 포트포워딩을 통한 로컬에서의 접근이 아닌 외부 도메인에서의 접근을 위해서 
grafana용 ingress를 생성해  Route53과 연동하여 grafanas.lovebridge.click 에서 접근 가능토록 하였습니다.

39. 위 사진은 default namespace를 모니터링한 지표입니다.
cpu 사용률과 cpu limit 대비 사용률 요청 메모리양 / 메모리 리미트 와 파드 같은 지표들을 확인할 수 있습니다.	